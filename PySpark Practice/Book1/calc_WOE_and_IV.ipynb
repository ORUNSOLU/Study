{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOE - Weight of Evidence\n",
    "\n",
    "IV - Information Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import QuantileDiscretizer, VectorAssembler\n",
    "import scipy.stats.stats as stats\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+---------------------+-------------+\n",
      "|Suburb    |Address         |Rooms|type|Price    |Method|SellerG|Date     |Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|CouncilArea|Lattitude|Longtitude|Regionname           |Propertycount|\n",
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+---------------------+-------------+\n",
      "|Abbotsford|85 Turner St    |2    |0   |1480000.0|S     |Biggin |3/12/2016|2.5     |3067.0  |2.0     |1.0     |1.0|202.0   |null        |null     |Yarra      |-37.7996 |144.9984  |Northern Metropolitan|4019.0       |\n",
      "|Abbotsford|25 Bloomburg St |2    |0   |1035000.0|S     |Biggin |4/02/2016|2.5     |3067.0  |2.0     |1.0     |0.0|156.0   |79.0        |1900.0   |Yarra      |-37.8079 |144.9934  |Northern Metropolitan|4019.0       |\n",
      "|Abbotsford|5 Charles St    |3    |0   |1465000.0|SP    |Biggin |4/03/2017|2.5     |3067.0  |3.0     |2.0     |0.0|134.0   |150.0       |1900.0   |Yarra      |-37.8093 |144.9944  |Northern Metropolitan|4019.0       |\n",
      "|Abbotsford|40 Federation La|3    |0   |850000.0 |PI    |Biggin |4/03/2017|2.5     |3067.0  |3.0     |2.0     |1.0|94.0    |null        |null     |Yarra      |-37.7969 |144.9969  |Northern Metropolitan|4019.0       |\n",
      "|Abbotsford|55a Park St     |4    |0   |1600000.0|VB    |Nelson |4/06/2016|2.5     |3067.0  |3.0     |1.0     |2.0|120.0   |142.0       |2014.0   |Yarra      |-37.8072 |144.9941  |Northern Metropolitan|4019.0       |\n",
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+---------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "dataset = 2 # load either Melbourne Housing dataset (2) or Bank dataset(1)\n",
    "\n",
    "# if option is set as 1, load bank dataset\n",
    "if dataset == 1:\n",
    "    filename = 'bank/bank-full.csv'\n",
    "    target_variable_name = 'y'\n",
    "    df = spark.read.csv(filename, header=True, inferSchema=True, sep=';')\n",
    "    df = df.withColumn(target_variable_name, F.when(df['target_variable_name'] == 'no', \n",
    "                                            0).otherwise(1))\n",
    "else:\n",
    "    filename = 'melb_data.csv'\n",
    "    target_variable_name = 'type'\n",
    "    df = spark.read.csv(filename, header=True, inferSchema=True, sep=',')\n",
    "    df = df.withColumn(target_variable_name, F.when(df[target_variable_name] == 'h',\n",
    "                                             0).otherwise(1))\n",
    "\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|type|count|\n",
      "+----+-----+\n",
      "|   1| 4131|\n",
      "|   0| 9449|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(target_variable_name).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Variable: ['Suburb', 'Address', 'Method', 'SellerG', 'Date', 'CouncilArea', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# identify variable types and perform some operations\n",
    "def variable_type(df):\n",
    "    vars_list = df.dtypes\n",
    "    char_vars, num_vars = [], []\n",
    "    for i in vars_list:\n",
    "        char_vars.append(i[0]) if i[1] in ('string') else num_vars.append(i[0])\n",
    "    return char_vars, num_vars\n",
    "\n",
    "char_vars, num_vars = variable_type(df)\n",
    "print('Character Variable: {}'.format(char_vars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Method',\n",
       " 'CouncilArea',\n",
       " 'Regionname',\n",
       " 'Rooms',\n",
       " 'Price',\n",
       " 'Distance',\n",
       " 'Postcode',\n",
       " 'Bedroom2',\n",
       " 'Bathroom',\n",
       " 'Car',\n",
       " 'Landsize',\n",
       " 'BuildingArea',\n",
       " 'YearBuilt',\n",
       " 'Lattitude',\n",
       " 'Longtitude',\n",
       " 'Propertycount']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the final dataset\n",
    "if dataset != 1:\n",
    "    char_vars.remove('Address')\n",
    "    char_vars.remove('SellerG')\n",
    "    char_vars.remove('Date')\n",
    "    char_vars.remove('Suburb')\n",
    "\n",
    "num_vars.remove(target_variable_name)\n",
    "final_vars = char_vars + num_vars\n",
    "\n",
    "final_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rho = 1\n",
    "max_bin = 20\n",
    "\n",
    "def calculcate_woe(count_df, event_df, min_value, max_value, feature):\n",
    "    woe_df = pd.merge(left=count_df, right=event_df)\n",
    "    woe_df['min_value'], woe_df['max_value'] = min_value, max_value\n",
    "    woe_df['non_event'] = woe_df['count'] - woe_df['event']\n",
    "    woe_df['event_rate'] = woe_df['event']/woe_df['count']\n",
    "    woe_df['nonevent_rate'] = woe_df['non_event']/woe_df['count']\n",
    "    woe_df['dist_event'] = woe_df['event']/woe_df['event'].sum()\n",
    "    woe_df['dist_nonevent'] = woe_df['non_event']/woe_df['non_event'].sum()\n",
    "    woe_df['woe'] = np.log(woe_df['dist_event']/woe_df['dist_nonevent'])\n",
    "    woe_df['iv'] = (woe_df['dist_event'] - woe_df['dist_nonevent'])*woe_df['woe']\n",
    "    woe_df['varname'] = [feature]* len(woe_df)\n",
    "    woe_df = woe_df[['varname','min_value', 'max_value', 'count','event', 'non_event', \n",
    "            'event_rate', 'nonevent_rate', 'dist_event','dist_nonevent','woe', 'iv']]\n",
    "    # replace Positive & Negative Infinity values with zero\n",
    "    woe_df = woe_df.replace([np.inf, -np.inf], 0) \n",
    "    woe_df['iv'] = woe_df['iv'].sum()\n",
    "    return woe_df\n",
    "\n",
    "# monotonic binning function implemented alng with Spearman correlation\n",
    "def mono_bin(temp_df, feature, target, n = max_bin):\n",
    "    r = 0\n",
    "    while np.abs(r) < custom_rho and n > 1:\n",
    "        try:\n",
    "            # Quantile Discretizer cuts data into equal no of obervations\n",
    "            qds = QuantileDiscretizer(numBuckets=n, inputCol=feature, outputCol='buckets', \n",
    "                                        relativeError=0.01)\n",
    "            bucketizer = qds.fit(temp_df)\n",
    "            temp_df = bucketizer.transform(temp_df)\n",
    "            corr_df = temp_df.groupBy('buckets').agg({feature:'avg', target:'avg'}).toPandas()\n",
    "            corr_df.columns = ['buckets', feature, target]\n",
    "            r, p = stats.spearmanr(corr_df[feature], corr_df[target])\n",
    "            n = n - 1\n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "        return temp_df\n",
    "\n",
    "# excute WOE for all the variables in the Dataset\n",
    "def execute_woe(df, target):\n",
    "    count = -1\n",
    "    for feature in final_vars:\n",
    "        if feature != target:\n",
    "            count +=1\n",
    "            temp_df = df.select([feature, target])\n",
    "            # perform Monotonic binnning for numeric variables before WOE calc\n",
    "            if feature in num_vars:\n",
    "                temp_df = mono_bin(temp_df, feature, target, n=max_bin)\n",
    "                # group buckets in numerical\n",
    "                grouped = temp_df.groupBy('buckets')\n",
    "            else:\n",
    "                # just group categories in categorical\n",
    "                grouped = temp_df.groupby(feature)\n",
    "            # count and event value for each group\n",
    "            count_df = grouped.agg(F.count(target).alias('count')).toPandas()\n",
    "            event_df = grouped.agg(F.sum(target).alias('event')).toPandas()\n",
    "            # store min and max values for variables. For category, both takes the same value.\n",
    "            if feature in num_vars:\n",
    "                min_value = grouped.agg(F.min(feature).alias('min')).toPandas()['min']\n",
    "                max_value = grouped.agg(F.max(feature).alias('max')).toPandas()['max']\n",
    "            else:\n",
    "                min_value, max_value = count_df[feature], count_df[feature]\n",
    "\n",
    "            # calc WOE & IV\n",
    "            temp_woe_df = calculcate_woe(count_df, event_df, min_value, max_value, feature)\n",
    "            # final dataset creation\n",
    "            if count == 0:\n",
    "                final_woe_df = temp_woe_df\n",
    "            else:\n",
    "                final_woe_df = final_woe_df.append(temp_woe_df, ignore_index=True)\n",
    "\n",
    "        # seperate IV dataset creation\n",
    "        iv = pd.DataFrame({'IV': final_woe_df.groupby('varname').iv.max()})\n",
    "        iv = iv.reset_index()\n",
    "    return final_woe_df, iv\n",
    "\n",
    "output, iv = execute_woe(df, target_variable_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b7b5a710781a87842cfe8010c06442799678ef166246941c85da77d9a8410c1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
