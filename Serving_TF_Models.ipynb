{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries here\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 381s 2us/step\n",
      "170508288/170498071 [==============================] - 381s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# load and normalize images\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# class labels\n",
    "CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sample size:  50000\n",
      "Validation data sample size:  500\n",
      "Test data sample size:  9500\n"
     ]
    }
   ],
   "source": [
    "# 500 images for Validation\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((test_images[:500], test_labels[:500]))\n",
    "\n",
    "# the rest for Testing\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images[500:], test_labels[500:]))\n",
    "\n",
    "# prep train Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# sample size of each dataset\n",
    "train_dataset_size = len(list(train_dataset.as_numpy_iterator()))\n",
    "validation_dataset_size = len(list(validation_dataset.as_numpy_iterator()))\n",
    "test_dataset_size = len(list(test_dataset.as_numpy_iterator()))\n",
    "\n",
    "print('Training data sample size: ', train_dataset_size)\n",
    "print('Validation data sample size: ', validation_dataset_size)\n",
    "print('Test data sample size: ', test_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# define a Distribution strategy for training\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync)) # No GPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set BATCH SIZE for training\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "train_dataset = train_dataset.repeat().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(validation_dataset_size)\n",
    "test_dataset = test_dataset.batch(test_dataset_size)\n",
    "\n",
    "STEPS_PER_EPOCH = train_dataset_size // BATCH_SIZE_PER_REPLICA # 781\n",
    "VALIDATION_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCIFAR10-20220120-143834\n"
     ]
    }
   ],
   "source": [
    "# define the function to BUILD the model\n",
    "def build_model():\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Conv2D(\n",
    "                32, kernel_size=(3,3), activation='relu', name='conv_1',\n",
    "                kernel_initializer='glorot_uniform', padding='same', input_shape=(32, 32, 3)\n",
    "            ),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(\n",
    "                64, kernel_size=(3, 3), activation='relu', name='conv_2',\n",
    "                kernel_initializer='glorot_uniform', padding='same'\n",
    "            ),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(name='flat_1'),\n",
    "            layers.Dense(256, activation='relu', kernel_initializer='glorot_uniform', name='dense_64'),\n",
    "            layers.Dense(10, activation='softmax', name='custom_class')\n",
    "        ])\n",
    "        model.build([None, 32, 32, 3])\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "# invoke the model\n",
    "model = build_model()\n",
    "MODEL_NAME = 'myCIFAR10-{}'.format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "print(MODEL_NAME)\n",
    "\n",
    "checkpoint_dir = './' + MODEL_NAME\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt-{epoch}\")\n",
    "\n",
    "myCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, monitor='val_accuracy', mode='max',\n",
    "    save_weights_only=True, save_best_only=True\n",
    ")\n",
    "\n",
    "myCallbacks = [myCheckPoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 64s 77ms/step - loss: 1.3748 - accuracy: 0.5162 - val_loss: 1.1050 - val_accuracy: 0.5960\n",
      "Epoch 2/30\n",
      "781/781 [==============================] - 61s 79ms/step - loss: 1.0129 - accuracy: 0.6477 - val_loss: 0.9867 - val_accuracy: 0.6540\n",
      "Epoch 3/30\n",
      "781/781 [==============================] - 63s 81ms/step - loss: 0.8753 - accuracy: 0.6944 - val_loss: 0.9563 - val_accuracy: 0.6800\n",
      "Epoch 4/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.7631 - accuracy: 0.7337 - val_loss: 0.8867 - val_accuracy: 0.6780\n",
      "Epoch 5/30\n",
      "781/781 [==============================] - 62s 80ms/step - loss: 0.6605 - accuracy: 0.7710 - val_loss: 0.8766 - val_accuracy: 0.7060\n",
      "Epoch 6/30\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.5688 - accuracy: 0.8014 - val_loss: 0.9317 - val_accuracy: 0.6940\n",
      "Epoch 7/30\n",
      "781/781 [==============================] - 71s 91ms/step - loss: 0.4799 - accuracy: 0.8338 - val_loss: 1.0433 - val_accuracy: 0.6600\n",
      "Epoch 8/30\n",
      "781/781 [==============================] - 61s 79ms/step - loss: 0.3930 - accuracy: 0.8661 - val_loss: 0.9984 - val_accuracy: 0.7040\n",
      "Epoch 9/30\n",
      "781/781 [==============================] - 61s 77ms/step - loss: 0.3157 - accuracy: 0.8908 - val_loss: 1.0562 - val_accuracy: 0.7100\n",
      "Epoch 10/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.2470 - accuracy: 0.9141 - val_loss: 1.2624 - val_accuracy: 0.6840\n",
      "Epoch 11/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.1862 - accuracy: 0.9365 - val_loss: 1.3377 - val_accuracy: 0.6880\n",
      "Epoch 12/30\n",
      "781/781 [==============================] - 61s 78ms/step - loss: 0.1462 - accuracy: 0.9507 - val_loss: 1.4335 - val_accuracy: 0.6860\n",
      "Epoch 13/30\n",
      "781/781 [==============================] - 61s 77ms/step - loss: 0.1142 - accuracy: 0.9617 - val_loss: 1.5632 - val_accuracy: 0.6900\n",
      "Epoch 14/30\n",
      "781/781 [==============================] - 60s 77ms/step - loss: 0.1033 - accuracy: 0.9660 - val_loss: 1.6869 - val_accuracy: 0.6860\n",
      "Epoch 15/30\n",
      "781/781 [==============================] - 61s 78ms/step - loss: 0.0760 - accuracy: 0.9753 - val_loss: 1.8157 - val_accuracy: 0.6880\n",
      "Epoch 16/30\n",
      "781/781 [==============================] - 62s 80ms/step - loss: 0.0895 - accuracy: 0.9695 - val_loss: 1.9652 - val_accuracy: 0.6920\n",
      "Epoch 17/30\n",
      "781/781 [==============================] - 63s 80ms/step - loss: 0.0723 - accuracy: 0.9757 - val_loss: 1.9685 - val_accuracy: 0.6800\n",
      "Epoch 18/30\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.0647 - accuracy: 0.9781 - val_loss: 2.1733 - val_accuracy: 0.7060\n",
      "Epoch 19/30\n",
      "781/781 [==============================] - 72s 92ms/step - loss: 0.0604 - accuracy: 0.9794 - val_loss: 2.1614 - val_accuracy: 0.6780\n",
      "Epoch 20/30\n",
      "781/781 [==============================] - 72s 93ms/step - loss: 0.0664 - accuracy: 0.9778 - val_loss: 2.3300 - val_accuracy: 0.6580\n",
      "Epoch 21/30\n",
      "781/781 [==============================] - 64s 82ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 2.4616 - val_accuracy: 0.6780\n",
      "Epoch 22/30\n",
      "781/781 [==============================] - 61s 79ms/step - loss: 0.0566 - accuracy: 0.9807 - val_loss: 2.3915 - val_accuracy: 0.6580\n",
      "Epoch 23/30\n",
      "781/781 [==============================] - 61s 79ms/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 2.5656 - val_accuracy: 0.6920\n",
      "Epoch 24/30\n",
      "781/781 [==============================] - 62s 80ms/step - loss: 0.0554 - accuracy: 0.9809 - val_loss: 2.5470 - val_accuracy: 0.6760\n",
      "Epoch 25/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.0535 - accuracy: 0.9823 - val_loss: 2.6784 - val_accuracy: 0.6900\n",
      "Epoch 26/30\n",
      "781/781 [==============================] - 63s 81ms/step - loss: 0.0470 - accuracy: 0.9843 - val_loss: 2.7805 - val_accuracy: 0.6620\n",
      "Epoch 27/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.0436 - accuracy: 0.9854 - val_loss: 2.7504 - val_accuracy: 0.6900\n",
      "Epoch 28/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.0465 - accuracy: 0.9841 - val_loss: 2.8698 - val_accuracy: 0.6900\n",
      "Epoch 29/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.0384 - accuracy: 0.9871 - val_loss: 2.8927 - val_accuracy: 0.6800\n",
      "Epoch 30/30\n",
      "781/781 [==============================] - 62s 79ms/step - loss: 0.0529 - accuracy: 0.9826 - val_loss: 2.9974 - val_accuracy: 0.6680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27887bcb320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(\n",
    "    train_dataset, epochs=30, steps_per_epoch=STEPS_PER_EPOCH, validation_data=validation_dataset,\n",
    "    validation_steps=VALIDATION_STEPS, callbacks=myCallbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./myCIFAR10-20220120-143834\\\\ckpt-9'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load the best weights to the model\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# check for latest model checkpoint in the DIR\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save MODEL to h5 format\n",
    "KERAS_MODEL_PATH = r'C:\\Users\\DELL\\Desktop\\Learning projects\\assests\\tfkeras_cifar10.h5'\n",
    "model.save(KERAS_MODEL_PATH)\n",
    "\n",
    "# to reload model\n",
    "new_h5_model = models.load_model(KERAS_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.01132735e-05, 2.75433422e-07, 4.76483814e-02, ...,\n",
       "        4.20681387e-03, 3.71989408e-08, 6.65380696e-07],\n",
       "       [3.07483482e-04, 5.94857283e-07, 9.25968122e-03, ...,\n",
       "        2.06165605e-05, 2.26582819e-09, 4.09300718e-08],\n",
       "       [2.07293968e-04, 4.43747723e-08, 4.47228970e-03, ...,\n",
       "        1.50847882e-01, 5.35844767e-04, 2.81508278e-06],\n",
       "       ...,\n",
       "       [6.41857014e-06, 1.19893956e-11, 1.20093515e-02, ...,\n",
       "        1.95345515e-03, 9.74205591e-07, 3.95296702e-08],\n",
       "       [1.71055943e-01, 4.78979856e-01, 5.90977678e-03, ...,\n",
       "        2.53810049e-06, 6.19740661e-07, 6.93959095e-08],\n",
       "       [3.58529895e-09, 3.47679929e-06, 6.87571128e-06, ...,\n",
       "        9.99704897e-01, 9.30349664e-09, 2.19125639e-07]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9500 rows\n",
    "new_h5_model.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protobuf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\DELL\\Desktop\\Learning projects\\assests\\pb_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# to save model to PB format, specifically use this\n",
    "SAVED_MODEL_PATH = r'C:\\Users\\DELL\\Desktop\\Learning projects\\assests\\pb_model'\n",
    "tf.saved_model.save(model, SAVED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a Protobuf model\n",
    "# this would work better with a GPU \n",
    "load_strategy = tf.distribute.MirroredStrategy()\n",
    "with load_strategy.scope():\n",
    "    load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "    loaded_pb = models.load_model(SAVED_MODEL_PATH, options=load_options)\n",
    "\n",
    "\n",
    "# to predict with PB model\n",
    "loaded_pb.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model was served using Docker...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client code which should be in a diff Notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "test_images = test_images[500:510]\n",
    "\n",
    "DATA = json.dumps({\"instances\": test_images.tolist()})\n",
    "HEADERS = {'content-type': 'application/json'}\n",
    "\n",
    "# TFS would score the DATA and return the results as response\n",
    "response = requests.post('http://localhost:8501/v1/models/cifar10:predict', data=DATA, headers=HEADERS)\n",
    "\n",
    "# to see raw response\n",
    "predictions_prob_list = response.json().get('predictions')\n",
    "CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "predictions_array = np.asarray(predictions_prob_list)\n",
    "predictions_idx = np.argmax(predictions_array, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b7b5a710781a87842cfe8010c06442799678ef166246941c85da77d9a8410c1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
